{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using LSTMs in Keras: \n",
    "\n",
    "Let's build an LSTM model that takes word **sequences** as input!\n",
    "Packages used here with keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are converting all the sentences to respective aray of indices .\n",
    "\n",
    "For this we are zero padding so that all arrays are of same size i.e the maximum length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for idx, val in enumerate([\"I\", \"like\", \"learning\"]):\n",
    "    print(idx,val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    m = X.shape[0]                                   \n",
    "    X_indices = np.zeros((m,max_len))\n",
    "    for i in range(m):                               \n",
    "        sentence_words =X[i].lower().split()\n",
    "        for w in sentence_words:\n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            j = j+1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 = ['funny lol' 'lets play baseball' 'food is ready for you']\n",
      "X1_indices =\n",
      " [[ 155345.  225122.       0.       0.       0.]\n",
      " [ 220930.  286375.   69714.       0.       0.]\n",
      " [ 151204.  192973.  302254.  151349.  394475.]]\n"
     ]
    }
   ],
   "source": [
    "X1 = np.array([\"funny lol\", \"lets play baseball\", \"food is ready for you\"])\n",
    "X1_indices = sentences_to_indices(X1,word_to_index, max_len = 5)\n",
    "print(\"X1 =\", X1)\n",
    "print(\"X1_indices =\\n\", X1_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "```Python\n",
    "X1 = ['funny lol' 'lets play baseball' 'food is ready for you']\n",
    "X1_indices =\n",
    " [[ 155345.  225122.       0.       0.       0.]\n",
    " [ 220930.  286375.   69714.       0.       0.]\n",
    " [ 151204.  192973.  302254.  151349.  394475.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are creating an embedding layer in keras with pretrained GloVe Vector values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    vocab_len = len(word_to_index) + 1                  \n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      \n",
    "    emb_matrix = np.zeros((vocab_len,emb_dim))\n",
    "    for word, idx in word_to_index.items():\n",
    "        emb_matrix[idx, :] = word_to_vec_map[word]\n",
    "    embedding_layer = Embedding(vocab_len,emb_dim)\n",
    "    embedding_layer.build((None,)) \n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights[0][1][3] = -0.3403\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "```Python\n",
    "weights[0][1][3] = -0.3403\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create the final model for our application with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Emojify_V2(input_shape, word_to_vec_map, word_to_index):\n",
    "    sentence_indices = Input(input_shape,dtype='int32')\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map,word_to_index)\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    X = LSTM(128,return_sequences=True)(embeddings)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = LSTM(128,return_sequences=False)(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(5)(X)\n",
    "    X = Activation(\"softmax\")(X)\n",
    "    model = Model(sentence_indices,X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 10, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 10, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 20,223,927\n",
      "Trainable params: 20,223,927\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "Y_train_oh = convert_to_one_hot(Y_train, C = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are fitting our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "132/132 [==============================] - 4s - loss: 1.6099 - acc: 0.2652     \n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 4s - loss: 1.5415 - acc: 0.3030     \n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 4s - loss: 1.4915 - acc: 0.3788     \n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 4s - loss: 1.4391 - acc: 0.3561     \n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 3s - loss: 1.3471 - acc: 0.4773     \n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 3s - loss: 1.2378 - acc: 0.4318     \n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 4s - loss: 1.0574 - acc: 0.6364     \n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 3s - loss: 0.9241 - acc: 0.6894     \n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 3s - loss: 0.7611 - acc: 0.6970     \n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 3s - loss: 0.7925 - acc: 0.6970     \n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 3s - loss: 0.5259 - acc: 0.8258     \n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 4s - loss: 0.5590 - acc: 0.8106     \n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 4s - loss: 0.4630 - acc: 0.8561     \n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 3s - loss: 0.3935 - acc: 0.8636     \n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 4s - loss: 0.2203 - acc: 0.9470     \n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 4s - loss: 0.2304 - acc: 0.9242     \n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 3s - loss: 0.1587 - acc: 0.9621     \n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 3s - loss: 0.1264 - acc: 0.9621     \n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 4s - loss: 0.1674 - acc: 0.9470     \n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 3s - loss: 0.0618 - acc: 0.9848     \n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 3s - loss: 0.0955 - acc: 0.9697     \n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 4s - loss: 0.1407 - acc: 0.9470     \n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 4s - loss: 0.1367 - acc: 0.9621     \n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 3s - loss: 0.0744 - acc: 0.9848     \n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 3s - loss: 0.0343 - acc: 1.0000     \n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 3s - loss: 0.0168 - acc: 1.0000     \n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 3s - loss: 0.0343 - acc: 0.9924     \n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 3s - loss: 0.5148 - acc: 0.9242     \n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 3s - loss: 0.7086 - acc: 0.8485     \n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 4s - loss: 0.3382 - acc: 0.8636     \n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 4s - loss: 0.2326 - acc: 0.8939     \n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 4s - loss: 0.1312 - acc: 0.9470     \n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 4s - loss: 0.0892 - acc: 0.9773     \n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 4s - loss: 0.0657 - acc: 0.9924     \n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 4s - loss: 0.0410 - acc: 1.0000     \n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 4s - loss: 0.0281 - acc: 1.0000     \n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 4s - loss: 0.0258 - acc: 1.0000     \n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 3s - loss: 0.0235 - acc: 0.9924     \n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 3s - loss: 0.0138 - acc: 1.0000     \n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 3s - loss: 0.0114 - acc: 1.0000     \n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 3s - loss: 0.0095 - acc: 1.0000     \n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 3s - loss: 0.0065 - acc: 1.0000     \n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 3s - loss: 0.0057 - acc: 1.0000     \n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 4s - loss: 0.0051 - acc: 1.0000     \n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 4s - loss: 0.0060 - acc: 1.0000     \n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 3s - loss: 0.0034 - acc: 1.0000     \n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 3s - loss: 0.0045 - acc: 1.0000     \n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 3s - loss: 0.0036 - acc: 1.0000     \n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 3s - loss: 0.0048 - acc: 1.0000     \n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 3s - loss: 0.0041 - acc: 1.0000     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f28cd91f080>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/56 [================>.............] - ETA: 0s\n",
      "Test accuracy =  0.803571428571\n"
     ]
    }
   ],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
    "Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected emoji:😄 prediction: he got a very nice raise\t❤️\n",
      "Expected emoji:😄 prediction: she got me a nice present\t❤️\n",
      "Expected emoji:😞 prediction: work is hard\t😄\n",
      "Expected emoji:😞 prediction: This girl is messing with me\t❤️\n",
      "Expected emoji:😞 prediction: work is horrible\t😄\n",
      "Expected emoji:🍴 prediction: any suggestions for dinner\t😄\n",
      "Expected emoji:😄 prediction: you brighten my day\t❤️\n",
      "Expected emoji:😞 prediction: she is a bully\t❤️\n",
      "Expected emoji:😞 prediction: My life is so boring\t❤️\n",
      "Expected emoji:😄 prediction: will you be my valentine\t❤️\n",
      "Expected emoji:😞 prediction: go away\t⚾\n"
     ]
    }
   ],
   "source": [
    "C = 5\n",
    "y_test_oh = np.eye(C)[Y_test.reshape(-1)]\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
    "pred = model.predict(X_test_indices)\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test_indices\n",
    "    num = np.argmax(pred[i])\n",
    "    if(num != Y_test[i]):\n",
    "        print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can try it on your own example. Write your own sentence below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not feeling happy 😞\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array(['not feeling happy'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "RNnEs",
   "launcher_item_id": "acNYU"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
